{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuPuPY377BAh",
        "outputId": "8419b728-ab84-49a7-cac1-0380328aa3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdCoQt64xG41"
      },
      "source": [
        "# **Define path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay6J8pY37CJ1"
      },
      "outputs": [],
      "source": [
        "ROOT_PROJECT = \"/content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221\" # path to project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnhHznq6xTE5"
      },
      "source": [
        "# **Clone repo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0DY1xLD7eya",
        "outputId": "1a730c44-4054-4015-8fd9-d4dc380389d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project_CS221\n",
            "Cloning into 'SKS'...\n",
            "remote: Enumerating objects: 305, done.\u001b[K\n",
            "remote: Counting objects: 100% (305/305), done.\u001b[K\n",
            "remote: Compressing objects: 100% (217/217), done.\u001b[K\n",
            "remote: Total 305 (delta 162), reused 221 (delta 80), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (305/305), 7.33 MiB | 6.98 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}\n",
        "!git clone https://github.com/bmhungqb/SKS.git SKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej68d1Ug8f4Z"
      },
      "source": [
        "# **Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhG1mabr7qVX",
        "outputId": "02b2c7a6-845d-4b05-a35e-e75879672fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1KUZ_9VBn505rUzXezbc-IZx3GjFFIvKL/Project_CS221/code/Project_CS221\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 1)) (3.8.1)\n",
            "Collecting numpy==1.26.2 (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 3)) (1.5.3)\n",
            "Collecting pyenchant==3.2.2 (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 4))\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn==1.3.2 (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 5))\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 6)) (1.11.4)\n",
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (2.15.0)\n",
            "Collecting tensorflow_intel (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading tensorflow_intel-0.0.1-py3-none-any.whl (7.1 kB)\n",
            "Collecting wordninja==2.0.0 (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 9))\n",
            "  Downloading wordninja-2.0.0.tar.gz (541 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.6/541.6 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langdetect==1.0.9 (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 10))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio (from -r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading gradio-4.12.0-py3-none-any.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 1)) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==1.3.2->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (2.15.0)\n",
            "Collecting pyinstaller (from tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading pyinstaller-6.3.0-py3-none-manylinux2014_x86_64.whl (662 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.0/663.0 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting twine (from tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading twine-4.0.2-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (1.0.3)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (4.2.2)\n",
            "Collecting fastapi (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.8.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading gradio_client-0.8.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (6.0.1)\n",
            "Collecting semantic-version~=2.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.8.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.1.1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic>=2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.6.6 (from tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (3.0.1)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (2.0.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (1.3.0)\n",
            "Collecting altgraph (from pyinstaller->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading altgraph-0.17.4-py2.py3-none-any.whl (21 kB)\n",
            "Collecting pyinstaller-hooks-contrib>=2021.4 (from pyinstaller->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading pyinstaller_hooks_contrib-2023.11-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pkginfo>=1.8.1 (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading readme_renderer-42.0-py3-none-any.whl (13 kB)\n",
            "Collecting requests-toolbelt!=0.9.0,>=0.8.0 (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (2.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.10/dist-packages (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (7.0.0)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/lib/python3/dist-packages (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (23.5.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=3.6->twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (3.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.15.2)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8))\n",
            "  Downloading nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (0.18.1)\n",
            "Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->tensorflow_intel->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 8)) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 11)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r /content/drive/MyDrive/UIT/Year3/HKI/CS221_NLP/Project_CS221/code/Project_CS221/SKS/requirements.txt (line 7)) (3.2.2)\n",
            "Building wheels for collected packages: wordninja, langdetect, ffmpy\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-py3-none-any.whl size=541529 sha256=630c2885332098d12988bb5fe9382abea8531eaa13ad8fadfe7f0ebe0f864571\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/44/3a/f2a5c1859b8b541ded969b4cd12d0a58897f12408f4f51e084\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=8baa3c2d9d1d7ad362943f436ec8fb5f9a625ad1baed88a8ed51acdf688ea313\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=ed18363158259f40b1f38b05092d2c31bdca381d8b333bfe590c5f7de0f5b72a\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built wordninja langdetect ffmpy\n",
            "Installing collected packages: wordninja, pydub, nh3, ffmpy, altgraph, websockets, typing-extensions, tomlkit, shellingham, semantic-version, rfc3986, readme-renderer, python-multipart, pyinstaller-hooks-contrib, pyenchant, pkginfo, orjson, numpy, langdetect, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, requests-toolbelt, pyinstaller, pydantic-core, httpcore, twine, scikit_learn, pydantic, httpx, tensorflow_intel, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 altgraph-0.17.4 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.105.0 ffmpy-0.3.1 gradio-4.12.0 gradio-client-0.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 langdetect-1.0.9 nh3-0.2.15 numpy-1.26.2 orjson-3.9.10 pkginfo-1.9.6 pydantic-2.5.3 pydantic-core-2.14.6 pydub-0.25.1 pyenchant-3.2.2 pyinstaller-6.3.0 pyinstaller-hooks-contrib-2023.11 python-multipart-0.0.6 readme-renderer-42.0 requests-toolbelt-1.0.0 rfc3986-2.0.0 scikit_learn-1.3.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.27.0 tensorflow_intel-0.0.1 tomlkit-0.12.0 twine-4.0.2 typing-extensions-4.9.0 uvicorn-0.25.0 websockets-11.0.3 wordninja-2.0.0\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}\n",
        "!pip install -r {ROOT_PROJECT}/SKS/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCFKgwzT8SXh",
        "outputId": "0cf2a453-db44-410c-89fb-b8c2c2a25183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader 'punkt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngkEp5Es8VKT",
        "outputId": "1edf688c-fef4-41c2-c1a1-cc555a61bc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [48.6 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,247 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,153 kB]\n",
            "Fetched 8,976 kB in 2s (3,595 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common enchant-2 hunspell-en-us libaspell15\n",
            "  libenchant-2-2 libhunspell-1.7-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-2-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common enchant-2 hunspell-en-us libaspell15\n",
            "  libenchant-2-2 libhunspell-1.7-0 libtext-iconv-perl python3-enchant\n",
            "0 upgraded, 10 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 1,464 kB of archives.\n",
            "After this operation, 5,673 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaspell15 amd64 0.60.8-4build1 [325 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell amd64 0.60.8-4build1 [87.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libenchant-2-2 amd64 2.3.2-1ubuntu2 [50.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 enchant-2 amd64 2.3.2-1ubuntu2 [13.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-enchant all 3.2.0-1 [33.8 kB]\n",
            "Fetched 1,464 kB in 1s (1,055 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../2-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../3-aspell_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking aspell (0.60.8-4build1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../4-aspell-en_2018.04.16-0-1_all.deb ...\n",
            "Unpacking aspell-en (2018.04.16-0-1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../5-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../6-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libenchant-2-2:amd64.\n",
            "Preparing to unpack .../7-libenchant-2-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Selecting previously unselected package enchant-2.\n",
            "Preparing to unpack .../8-enchant-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking enchant-2 (2.3.2-1ubuntu2) ...\n",
            "Selecting previously unselected package python3-enchant.\n",
            "Preparing to unpack .../9-python3-enchant_3.2.0-1_all.deb ...\n",
            "Unpacking python3-enchant (3.2.0-1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Setting up aspell (0.60.8-4build1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Setting up aspell-en (2018.04.16-0-1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up enchant-2 (2.3.2-1ubuntu2) ...\n",
            "Setting up python3-enchant (3.2.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install python3-enchant -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc6qz-qkzYh1"
      },
      "source": [
        "# **Download data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEeJkR_wl_Pw",
        "outputId": "717a3c1b-6b78-4ea2-f416-bd94c65a2716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1fsGCaPYe4CP4WYC0FCHd3HuqJ6TOUA1j/Project_CS221/SKS/data\n",
            "--2023-12-20 17:54:05--  https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.90, 3.163.189.74, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/stanfordnlp/glove/6471382cdd837544bf3ac72497a38715e845897d265b2b424b4761832009c837?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27glove.6B.zip%3B+filename%3D%22glove.6B.zip%22%3B&response-content-type=application%2Fzip&Expires=1703354045&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzM1NDA0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zdGFuZm9yZG5scC9nbG92ZS82NDcxMzgyY2RkODM3NTQ0YmYzYWM3MjQ5N2EzODcxNWU4NDU4OTdkMjY1YjJiNDI0YjQ3NjE4MzIwMDljODM3P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=ne2rOOatPh69yjDfMdKGtYsP-8XsHqr6zYxw2ZZbpH8%7Ema9uwdcU4nvXRhsASJTTJWTKGCPw-owvA8whmewuVjCiSMIvQT3UYzf5qKZS8h6I3FwBSHRTaXvcuZ69dNkevLDy4bYFKFsSJuZaucqug6C5SK8qkl1as5QuSHewpt0b58nxBEyke-81NCOjDIKGpzAgkEJBN1OnQQCV709savrJjqt8C4MxVKSSbtSuLhMlOha2Z1pQq7horFqx5OuyLFLUC5XvAzLjDtUGcFPcWuh8NVmRjzqMiRW0jZQNPHig1lzedBRY%7ETmtSAPwd1I7LQget5vfkEv3NXxmJjhTfg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-20 17:54:05--  https://cdn-lfs.huggingface.co/stanfordnlp/glove/6471382cdd837544bf3ac72497a38715e845897d265b2b424b4761832009c837?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27glove.6B.zip%3B+filename%3D%22glove.6B.zip%22%3B&response-content-type=application%2Fzip&Expires=1703354045&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMzM1NDA0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zdGFuZm9yZG5scC9nbG92ZS82NDcxMzgyY2RkODM3NTQ0YmYzYWM3MjQ5N2EzODcxNWU4NDU4OTdkMjY1YjJiNDI0YjQ3NjE4MzIwMDljODM3P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=ne2rOOatPh69yjDfMdKGtYsP-8XsHqr6zYxw2ZZbpH8%7Ema9uwdcU4nvXRhsASJTTJWTKGCPw-owvA8whmewuVjCiSMIvQT3UYzf5qKZS8h6I3FwBSHRTaXvcuZ69dNkevLDy4bYFKFsSJuZaucqug6C5SK8qkl1as5QuSHewpt0b58nxBEyke-81NCOjDIKGpzAgkEJBN1OnQQCV709savrJjqt8C4MxVKSSbtSuLhMlOha2Z1pQq7horFqx5OuyLFLUC5XvAzLjDtUGcFPcWuh8NVmRjzqMiRW0jZQNPHig1lzedBRY%7ETmtSAPwd1I7LQget5vfkEv3NXxmJjhTfg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.23, 108.138.94.122, 108.138.94.14, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  62.8MB/s    in 13s     \n",
            "\n",
            "2023-12-20 17:54:19 (61.4 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS/data'\n",
        "!wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIQtciD-9fWr"
      },
      "source": [
        "# **Process data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBqFM7Z5A6UO"
      },
      "source": [
        "## Dataset: SemEval_task5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h87AAp99p-H",
        "outputId": "658719ed-2892-48c9-bb5d-7dcd5fbbc2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project_CS221/SKS\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS'\n",
        "!python process_data/refactor_SemEval_task5.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K5gOoGOBAQZ"
      },
      "source": [
        "## Dataset: Davidson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH77aFPzBDUV",
        "outputId": "7d198eef-5c80-4406-f744-e8987a11ad2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project_CS221/SKS\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS'\n",
        "!python process_data/refactor_davidson.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv3kocuM8mLK"
      },
      "source": [
        "# **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOxH75tzXShf"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8keKBPn8pLR"
      },
      "source": [
        "## Dataset: SemEval_task5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Iekujlt82xi",
        "outputId": "fc173739-1148-471d-f348-d5e94e091a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project_CS221/SKS\n",
            "2023-12-24 08:41:06.564032: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-24 08:41:06.564086: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-24 08:41:06.565382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-24 08:41:06.572663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-24 08:41:07.598237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-24 08:41:09.367300: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:41:09.414263: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:41:09.414592: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:41:09.601439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:41:09.601780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:41:09.602011: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:41:09.602145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\u001b[94m[INFO]\u001b[0m (utils) Arguments:\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   batch_size: 512\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   command: DNN/train.py -d data/SemEval_task5/df_train_dev.csv --trial data/SemEval_task5/df_test.csv -s data/sentiment_datasets/train_E6oV3lV.csv --word_list data/word_list/word_all.txt --emb data/glove.6B.300d.txt -o outputs/SemEval -b 512 --epochs 50 --lr 0.002 --maxlen 50 -t HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   cross_validation: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   data_path: data/SemEval_task5/df_train_dev.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   dropout_prob: 0.1\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   emb_dim: 300\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   emb_path: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   epochs: 50\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   humor_data_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   learn_rate: 0.002\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   loss: ce\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   maxlen: 50\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   model_retrain_cross_validation_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   model_type: HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   non_gate: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   out_dir_path: outputs/SemEval\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   retrain_cross_validation: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   sarcasm_data_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   sentiment_data_path: data/sentiment_datasets/train_E6oV3lV.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   trial_data_path: data/SemEval_task5/df_test.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   vocab_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   word_list_path: data/word_list/word_all.txt\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   word_norm: 1\n",
            "test_task size>>> 3000\n",
            "test_task size>>> 3320\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) Creating vocabulary.........\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader)   487488 total words, 41385 unique words\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader)   Vocab size: 41385\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) <unk> hit rate: 0.00%\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) <unk> hit rate: 0.01%\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Statistics:\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   train_x shape: (30000, 50)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   test_x shape:  (3320, 50)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   train_chars shape: (30000, 300)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   test_chars shape:  (3320, 300)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   train_y shape: (30000, 2)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   test_y shape:  (3320, 2)\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "2023-12-24 08:42:03.187441: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.187887: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.188133: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.188659: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.188972: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.189187: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.189458: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.189671: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-24 08:42:03.189848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1241550   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 50, 100)              4138500   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 50, 400)              0         ['emb[0][0]',                 \n",
            "                                                                     'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " hsmm_bottom (HSMMBottom)    (None, 2, 150)               2916906   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 150)                  0         ['hsmm_bottom[0][0]']         \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 150)                  0         ['hsmm_bottom[0][0]']         \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower (HSMMTower)      (None, 2)                    7652      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " hsmm_tower_1 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack (TFOpLambda)       (None, 2, 2)                 0         ['hsmm_tower[0][0]',          \n",
            "                                                                     'hsmm_tower_1[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLamb  (None, 2, 1)                 0         ['tf.stack[0][0]',            \n",
            " da)                                                                 'tf.expand_dims[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze (TFOp  (None, 2)                    0         ['tf.linalg.matmul[0][0]']    \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19486210 (74.33 MB)\n",
            "Trainable params: 19486210 (74.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 20107/41385 word vectors initialized (hit rate: 48.59%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture\n",
            "2023-12-24 08:43:04.545967: I external/local_xla/xla/service/service.cc:168] XLA service 0x7dc5810603f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-24 08:43:04.546074: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-12-24 08:43:04.604673: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-12-24 08:43:04.714923: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1703407384.929918    4478 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "59/59 [==============================] - 37s 315ms/step - loss: 0.4002 - accuracy: 0.8211 - val_loss: 0.6698 - val_accuracy: 0.5864\n",
            "7/7 [==============================] - 2s 75ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 0, train: 49s, evaluation: 14s, total_time: 63s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.4002, metric: 0.8211\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.554667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.543944 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.554200 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.564641 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 317ms/step - loss: 0.3509 - accuracy: 0.8366 - val_loss: 0.6534 - val_accuracy: 0.5801\n",
            "7/7 [==============================] - 1s 80ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 1, train: 20s, evaluation: 12s, total_time: 97s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.3509, metric: 0.8366\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.547667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.546978 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.551183 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.579068 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 18s 310ms/step - loss: 0.3352 - accuracy: 0.8433 - val_loss: 0.6512 - val_accuracy: 0.5786\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 2, train: 18s, evaluation: 1s, total_time: 117s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.3352, metric: 0.8433\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.546000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.540302 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.559887 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.578515 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 20s 339ms/step - loss: 0.3187 - accuracy: 0.8496 - val_loss: 0.6949 - val_accuracy: 0.5855\n",
            "7/7 [==============================] - 1s 90ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 3, train: 20s, evaluation: 12s, total_time: 150s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.3187, metric: 0.8496\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.553000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.549156 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.563504 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.585542 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 325ms/step - loss: 0.3116 - accuracy: 0.8575 - val_loss: 0.7311 - val_accuracy: 0.5867\n",
            "7/7 [==============================] - 1s 92ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 4, train: 20s, evaluation: 2s, total_time: 172s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.3116, metric: 0.8575\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.554000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.546805 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.570331 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.586372 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 326ms/step - loss: 0.2963 - accuracy: 0.8671 - val_loss: 0.7107 - val_accuracy: 0.6021\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 5, train: 20s, evaluation: 13s, total_time: 206s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2963, metric: 0.8671\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.570667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.566093 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.582368 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.602069 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 319ms/step - loss: 0.2882 - accuracy: 0.8720 - val_loss: 0.8965 - val_accuracy: 0.5702\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 6, train: 20s, evaluation: 1s, total_time: 228s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2882, metric: 0.8720\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.535333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.512886 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.574511 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.564805 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 20s 337ms/step - loss: 0.2776 - accuracy: 0.8798 - val_loss: 0.9794 - val_accuracy: 0.5316\n",
            "7/7 [==============================] - 1s 89ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 7, train: 19s, evaluation: 2s, total_time: 250s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2776, metric: 0.8798\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.492000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.442976 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.567464 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.514871 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 20s 335ms/step - loss: 0.2711 - accuracy: 0.8804 - val_loss: 0.7719 - val_accuracy: 0.6084\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 8, train: 20s, evaluation: 11s, total_time: 282s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2711, metric: 0.8804\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.577000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.572163 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.589116 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.608359 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 317ms/step - loss: 0.2611 - accuracy: 0.8867 - val_loss: 0.7971 - val_accuracy: 0.5642\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 9, train: 20s, evaluation: 1s, total_time: 304s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2611, metric: 0.8867\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.530000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.505942 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.571501 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.557507 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 20s 336ms/step - loss: 0.2545 - accuracy: 0.8921 - val_loss: 0.9537 - val_accuracy: 0.5693\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 10, train: 20s, evaluation: 1s, total_time: 326s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2545, metric: 0.8921\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.532667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.508897 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.573723 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.562997 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 330ms/step - loss: 0.2477 - accuracy: 0.8950 - val_loss: 0.7288 - val_accuracy: 0.5967\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 11, train: 20s, evaluation: 1s, total_time: 348s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2477, metric: 0.8950\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.564000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.553125 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.586029 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.595262 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 319ms/step - loss: 0.2417 - accuracy: 0.8991 - val_loss: 0.7457 - val_accuracy: 0.5684\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 12, train: 20s, evaluation: 1s, total_time: 370s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2417, metric: 0.8991\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.532667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.505758 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.578027 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.560692 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 324ms/step - loss: 0.2344 - accuracy: 0.9023 - val_loss: 0.7930 - val_accuracy: 0.6078\n",
            "7/7 [==============================] - 1s 89ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 13, train: 19s, evaluation: 1s, total_time: 391s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2344, metric: 0.9023\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.576000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.570614 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.589081 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.607667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 327ms/step - loss: 0.2253 - accuracy: 0.9053 - val_loss: 1.0204 - val_accuracy: 0.5648\n",
            "7/7 [==============================] - 1s 90ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 14, train: 20s, evaluation: 1s, total_time: 413s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2253, metric: 0.9053\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.528667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.498481 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.578539 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.555742 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 328ms/step - loss: 0.2200 - accuracy: 0.9089 - val_loss: 0.8507 - val_accuracy: 0.6127\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 15, train: 20s, evaluation: 12s, total_time: 445s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2200, metric: 0.9089\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.580667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.575404 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.593493 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.612497 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 319ms/step - loss: 0.2147 - accuracy: 0.9108 - val_loss: 0.8458 - val_accuracy: 0.5467\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 16, train: 20s, evaluation: 1s, total_time: 467s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2147, metric: 0.9108\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.509000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.472116 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.568210 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.535061 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 20s 334ms/step - loss: 0.2046 - accuracy: 0.9167 - val_loss: 0.7559 - val_accuracy: 0.5937\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 17, train: 19s, evaluation: 1s, total_time: 488s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2046, metric: 0.9167\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.560333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.550442 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.580894 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.592436 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 327ms/step - loss: 0.1994 - accuracy: 0.9174 - val_loss: 0.8279 - val_accuracy: 0.6072\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 18, train: 20s, evaluation: 1s, total_time: 510s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1994, metric: 0.9174\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.575000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.568826 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.589430 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.606914 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 319ms/step - loss: 0.1927 - accuracy: 0.9223 - val_loss: 1.0958 - val_accuracy: 0.5383\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 19, train: 18s, evaluation: 1s, total_time: 531s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1927, metric: 0.9223\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.498333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.451921 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.570264 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.522393 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 323ms/step - loss: 0.1873 - accuracy: 0.9241 - val_loss: 1.0923 - val_accuracy: 0.5623\n",
            "7/7 [==============================] - 1s 87ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 20, train: 20s, evaluation: 1s, total_time: 553s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1873, metric: 0.9241\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.526333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.497807 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.574010 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.553897 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 325ms/step - loss: 0.1777 - accuracy: 0.9285 - val_loss: 0.7983 - val_accuracy: 0.6036\n",
            "7/7 [==============================] - 1s 87ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 21, train: 20s, evaluation: 1s, total_time: 575s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1777, metric: 0.9285\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.572000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.562503 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.591810 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.602366 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 324ms/step - loss: 0.1704 - accuracy: 0.9327 - val_loss: 0.9375 - val_accuracy: 0.5620\n",
            "7/7 [==============================] - 1s 84ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 22, train: 20s, evaluation: 1s, total_time: 597s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1704, metric: 0.9327\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.525000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.497732 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.570992 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.554363 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 322ms/step - loss: 0.1614 - accuracy: 0.9364 - val_loss: 1.4310 - val_accuracy: 0.5470\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 23, train: 19s, evaluation: 1s, total_time: 617s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1614, metric: 0.9364\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.508333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.467686 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.572516 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.533722 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 322ms/step - loss: 0.1554 - accuracy: 0.9377 - val_loss: 0.9433 - val_accuracy: 0.5858\n",
            "7/7 [==============================] - 1s 87ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 24, train: 19s, evaluation: 1s, total_time: 638s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1554, metric: 0.9377\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.551333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.534407 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.582463 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.582177 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 324ms/step - loss: 0.1458 - accuracy: 0.9417 - val_loss: 1.1340 - val_accuracy: 0.5669\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 25, train: 20s, evaluation: 1s, total_time: 660s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1458, metric: 0.9417\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.530333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.503697 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.575366 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.559547 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 322ms/step - loss: 0.1406 - accuracy: 0.9437 - val_loss: 1.3612 - val_accuracy: 0.5837\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 26, train: 19s, evaluation: 1s, total_time: 680s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1406, metric: 0.9437\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.548667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.529730 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.582702 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.579323 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 325ms/step - loss: 0.1321 - accuracy: 0.9475 - val_loss: 1.1696 - val_accuracy: 0.5708\n",
            "7/7 [==============================] - 1s 89ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 27, train: 20s, evaluation: 1s, total_time: 702s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1321, metric: 0.9475\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.535333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.512000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.575741 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.564871 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 323ms/step - loss: 0.1263 - accuracy: 0.9486 - val_loss: 1.2275 - val_accuracy: 0.5973\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 28, train: 19s, evaluation: 1s, total_time: 722s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1263, metric: 0.9486\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.562667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.550579 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.586547 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.595364 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 324ms/step - loss: 0.1164 - accuracy: 0.9534 - val_loss: 1.0739 - val_accuracy: 0.6392\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 29, train: 20s, evaluation: 12s, total_time: 755s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1164, metric: 0.9534\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.610000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.609989 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.609678 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.636790 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 319ms/step - loss: 0.1156 - accuracy: 0.9548 - val_loss: 0.9535 - val_accuracy: 0.5991\n",
            "7/7 [==============================] - 1s 93ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 30, train: 20s, evaluation: 2s, total_time: 778s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1156, metric: 0.9548\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.566333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.559071 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.582649 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.598490 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 330ms/step - loss: 0.1032 - accuracy: 0.9598 - val_loss: 1.4646 - val_accuracy: 0.5334\n",
            "7/7 [==============================] - 1s 87ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 31, train: 20s, evaluation: 1s, total_time: 800s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1032, metric: 0.9598\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.493000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.440999 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.572280 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.514538 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 320ms/step - loss: 0.0965 - accuracy: 0.9621 - val_loss: 1.7257 - val_accuracy: 0.6012\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 32, train: 20s, evaluation: 1s, total_time: 822s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0965, metric: 0.9621\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.568667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.560127 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.587012 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.600181 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 315ms/step - loss: 0.0949 - accuracy: 0.9641 - val_loss: 1.2817 - val_accuracy: 0.6072\n",
            "7/7 [==============================] - 1s 83ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 33, train: 20s, evaluation: 1s, total_time: 844s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0949, metric: 0.9641\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.575333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.571215 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.586175 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.607201 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 317ms/step - loss: 0.0889 - accuracy: 0.9660 - val_loss: 1.6851 - val_accuracy: 0.5786\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 34, train: 18s, evaluation: 1s, total_time: 864s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0889, metric: 0.9660\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.543667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.523512 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.579501 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.573629 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 323ms/step - loss: 0.0792 - accuracy: 0.9695 - val_loss: 1.6976 - val_accuracy: 0.5395\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 35, train: 20s, evaluation: 1s, total_time: 886s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0792, metric: 0.9695\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.499667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.451501 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.573838 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.522312 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 320ms/step - loss: 0.0748 - accuracy: 0.9712 - val_loss: 1.5917 - val_accuracy: 0.5997\n",
            "7/7 [==============================] - 1s 84ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 36, train: 20s, evaluation: 1s, total_time: 908s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0748, metric: 0.9712\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.564667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.555794 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.583585 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.598634 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 316ms/step - loss: 0.0745 - accuracy: 0.9717 - val_loss: 1.8241 - val_accuracy: 0.6027\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 37, train: 18s, evaluation: 1s, total_time: 928s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0745, metric: 0.9717\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.570667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.563053 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.587508 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.601972 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 320ms/step - loss: 0.0677 - accuracy: 0.9748 - val_loss: 1.4746 - val_accuracy: 0.6355\n",
            "7/7 [==============================] - 1s 89ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 38, train: 20s, evaluation: 1s, total_time: 950s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0677, metric: 0.9748\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.606000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.605863 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.607314 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.634161 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 318ms/step - loss: 0.0641 - accuracy: 0.9752 - val_loss: 1.4656 - val_accuracy: 0.6120\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 39, train: 20s, evaluation: 1s, total_time: 972s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0641, metric: 0.9752\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.580000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.576202 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.590217 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.612034 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 317ms/step - loss: 0.0638 - accuracy: 0.9762 - val_loss: 1.6813 - val_accuracy: 0.6437\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/SemEval/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc6304725c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630471270> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc63014e1d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630150460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7dc630152aa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 40, train: 20s, evaluation: 12s, total_time: 1005s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0638, metric: 0.9762\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.613667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.613485 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.612508 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.640270 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 314ms/step - loss: 0.0584 - accuracy: 0.9774 - val_loss: 2.6784 - val_accuracy: 0.5759\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 41, train: 18s, evaluation: 1s, total_time: 1025s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0584, metric: 0.9774\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.540000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.519889 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.575833 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.570801 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 327ms/step - loss: 0.0561 - accuracy: 0.9799 - val_loss: 2.4027 - val_accuracy: 0.5873\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 42, train: 20s, evaluation: 1s, total_time: 1047s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0561, metric: 0.9799\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.552667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.538255 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.580130 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.584602 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 316ms/step - loss: 0.0509 - accuracy: 0.9814 - val_loss: 2.0181 - val_accuracy: 0.5449\n",
            "7/7 [==============================] - 1s 90ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 43, train: 18s, evaluation: 1s, total_time: 1067s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0509, metric: 0.9814\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.505333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.467870 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.565387 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.532559 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 18s 311ms/step - loss: 0.0472 - accuracy: 0.9825 - val_loss: 2.3127 - val_accuracy: 0.5877\n",
            "7/7 [==============================] - 1s 88ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 44, train: 18s, evaluation: 1s, total_time: 1087s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0472, metric: 0.9825\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.552667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.540199 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.577248 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.585539 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 18s 314ms/step - loss: 0.0446 - accuracy: 0.9839 - val_loss: 2.0885 - val_accuracy: 0.6361\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 45, train: 20s, evaluation: 1s, total_time: 1109s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0446, metric: 0.9839\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.605667 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.605407 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.604306 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.632379 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 18s 314ms/step - loss: 0.0444 - accuracy: 0.9841 - val_loss: 2.9950 - val_accuracy: 0.6000\n",
            "7/7 [==============================] - 1s 90ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 46, train: 20s, evaluation: 1s, total_time: 1131s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0444, metric: 0.9841\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.565000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.554386 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.586618 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.598514 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 19s 317ms/step - loss: 0.0392 - accuracy: 0.9859 - val_loss: 2.6994 - val_accuracy: 0.5711\n",
            "7/7 [==============================] - 1s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 47, train: 18s, evaluation: 1s, total_time: 1151s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0392, metric: 0.9859\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.535333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.516240 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.569804 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.566430 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 18s 313ms/step - loss: 0.0404 - accuracy: 0.9865 - val_loss: 3.5779 - val_accuracy: 0.5470\n",
            "7/7 [==============================] - 1s 85ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 48, train: 20s, evaluation: 2s, total_time: 1174s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0404, metric: 0.9865\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.508333 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.470185 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.569228 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.534205 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "59/59 [==============================] - 18s 312ms/step - loss: 0.0403 - accuracy: 0.9866 - val_loss: 3.1073 - val_accuracy: 0.5910\n",
            "7/7 [==============================] - 1s 91ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 49, train: 18s, evaluation: 1s, total_time: 1193s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0403, metric: 0.9866\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.556000 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.544632 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.578879 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.589321 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training:   1031 seconds in total\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Evaluation: 162 seconds in total\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 40:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.613485 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.72      0.62      1325\n",
            "           1       0.70      0.53      0.61      1675\n",
            "\n",
            "    accuracy                           0.61      3000\n",
            "   macro avg       0.63      0.62      0.61      3000\n",
            "weighted avg       0.64      0.61      0.61      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS'\n",
        "output_path = ROOT_PROJECT+'/SKS/outputs/SemEval'\n",
        "if not os.path.exists(output_path):\n",
        "  os.makedirs(output_path)\n",
        "!python DNN/train.py -d data/SemEval_task5/df_train_dev.csv --trial data/SemEval_task5/df_test.csv -s data/sentiment_datasets/train_E6oV3lV.csv --word_list data/word_list/word_all.txt --emb data/glove.6B.300d.txt -o outputs/SemEval -b 512 --epochs 50 --lr 0.002 --maxlen 50 -t HHMM_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNyGvQ28sto"
      },
      "source": [
        "## Dataset: Davidson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWGgWffVImi7",
        "outputId": "404c4a7e-f98d-4590-fc21-22a063c4daab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1fsGCaPYe4CP4WYC0FCHd3HuqJ6TOUA1j/Project_CS221/SKS\n",
            "2023-12-21 03:47:59.893962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-21 03:47:59.894024: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-21 03:47:59.895856: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-21 03:47:59.906470: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-21 03:48:01.462295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-21 03:48:03.872579: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:48:03.918914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:48:03.919271: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:48:04.013239: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:48:04.013562: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:48:04.013782: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:48:04.013925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\u001b[94m[INFO]\u001b[0m (utils) Arguments:\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   batch_size: 512\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   command: DNN/train.py -d data/davidson/train_data.csv --trial data/davidson/test_data.csv -s data/sentiment_datasets/train_E6oV3lV.csv --word_list data/word_list/word_all.txt --emb data/glove.6B.300d.txt -o outputs/davidson -b 512 --epochs 30 --lr 0.002 --maxlen 50 -t HHMM_transformer --cross_validation\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   cross_validation: True\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   data_path: data/davidson/train_data.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   dropout_prob: 0.1\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   emb_dim: 300\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   emb_path: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   epochs: 30\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   humor_data_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   learn_rate: 0.002\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   loss: ce\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   maxlen: 50\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   model_type: HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   non_gate: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   out_dir_path: outputs/davidson\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   sarcasm_data_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   sentiment_data_path: data/sentiment_datasets/train_E6oV3lV.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   trial_data_path: data/davidson/test_data.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   vocab_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   word_list_path: data/word_list/word_all.txt\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   word_norm: 1\n",
            "test_task size>>> 4124\n",
            "test_task size>>> 4444\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) Creating vocabulary.........\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader)   622876 total words, 42793 unique words\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader)   Vocab size: 42793\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) <unk> hit rate: 0.00%\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) <unk> hit rate: 0.01%\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "2023-12-21 03:49:07.603085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.603499: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.603716: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.604196: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.604451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.604651: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.604913: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.605116: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 03:49:07.605282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1283790   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 50, 100)              4279300   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 50, 400)              0         ['emb[0][0]',                 \n",
            "                                                                     'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " hsmm_bottom (HSMMBottom)    (None, 2, 150)               2916906   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 150)                  0         ['hsmm_bottom[0][0]']         \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 150)                  0         ['hsmm_bottom[0][0]']         \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower (HSMMTower)      (None, 2)                    7652      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " hsmm_tower_1 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack (TFOpLambda)       (None, 2, 2)                 0         ['hsmm_tower[0][0]',          \n",
            "                                                                     'hsmm_tower_1[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLamb  (None, 2, 1)                 0         ['tf.stack[0][0]',            \n",
            " da)                                                                 'tf.expand_dims[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze (TFOp  (None, 2)                    0         ['tf.linalg.matmul[0][0]']    \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20049410 (76.48 MB)\n",
            "Trainable params: 20049410 (76.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 1/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 1/5\n",
            "Epoch 1/30\n",
            "2023-12-21 03:50:07.843118: I external/local_xla/xla/service/service.cc:168] XLA service 0x7d7bad2619a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-21 03:50:07.843184: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-12-21 03:50:07.902025: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-12-21 03:50:08.007521: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1703130608.126017    4258 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "76/76 [==============================] - 42s 333ms/step - loss: 0.2658 - accuracy: 0.9289 - val_loss: 0.2180 - val_accuracy: 0.9324\n",
            "Epoch 2/30\n",
            "76/76 [==============================] - 23s 297ms/step - loss: 0.2231 - accuracy: 0.9296 - val_loss: 0.1921 - val_accuracy: 0.9324\n",
            "Epoch 3/30\n",
            "76/76 [==============================] - 24s 317ms/step - loss: 0.2052 - accuracy: 0.9302 - val_loss: 0.1836 - val_accuracy: 0.9360\n",
            "Epoch 4/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1895 - accuracy: 0.9347 - val_loss: 0.1743 - val_accuracy: 0.9406\n",
            "Epoch 5/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1795 - accuracy: 0.9363 - val_loss: 0.1788 - val_accuracy: 0.9398\n",
            "Epoch 6/30\n",
            "76/76 [==============================] - 24s 317ms/step - loss: 0.1720 - accuracy: 0.9383 - val_loss: 0.1712 - val_accuracy: 0.9401\n",
            "Epoch 7/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1667 - accuracy: 0.9416 - val_loss: 0.1681 - val_accuracy: 0.9394\n",
            "Epoch 8/30\n",
            "76/76 [==============================] - 24s 310ms/step - loss: 0.1613 - accuracy: 0.9430 - val_loss: 0.1685 - val_accuracy: 0.9403\n",
            "Epoch 9/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1556 - accuracy: 0.9439 - val_loss: 0.1640 - val_accuracy: 0.9442\n",
            "Epoch 10/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1509 - accuracy: 0.9450 - val_loss: 0.1661 - val_accuracy: 0.9433\n",
            "Epoch 11/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.1475 - accuracy: 0.9462 - val_loss: 0.1569 - val_accuracy: 0.9437\n",
            "Epoch 12/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1427 - accuracy: 0.9485 - val_loss: 0.1755 - val_accuracy: 0.9310\n",
            "Epoch 13/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.1396 - accuracy: 0.9504 - val_loss: 0.1702 - val_accuracy: 0.9383\n",
            "Epoch 14/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1357 - accuracy: 0.9505 - val_loss: 0.1553 - val_accuracy: 0.9441\n",
            "Epoch 15/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1293 - accuracy: 0.9533 - val_loss: 0.1612 - val_accuracy: 0.9401\n",
            "Epoch 16/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1263 - accuracy: 0.9548 - val_loss: 0.1580 - val_accuracy: 0.9436\n",
            "Epoch 17/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1214 - accuracy: 0.9564 - val_loss: 0.1638 - val_accuracy: 0.9466\n",
            "Epoch 18/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1180 - accuracy: 0.9573 - val_loss: 0.1699 - val_accuracy: 0.9470\n",
            "Epoch 19/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1121 - accuracy: 0.9603 - val_loss: 0.1698 - val_accuracy: 0.9466\n",
            "Epoch 20/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1091 - accuracy: 0.9601 - val_loss: 0.1873 - val_accuracy: 0.9344\n",
            "Epoch 21/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1036 - accuracy: 0.9626 - val_loss: 0.1696 - val_accuracy: 0.9393\n",
            "Epoch 22/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0976 - accuracy: 0.9649 - val_loss: 0.1841 - val_accuracy: 0.9421\n",
            "Epoch 23/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0947 - accuracy: 0.9659 - val_loss: 0.2019 - val_accuracy: 0.9278\n",
            "Epoch 24/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0907 - accuracy: 0.9672 - val_loss: 0.1967 - val_accuracy: 0.9356\n",
            "Epoch 25/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.0848 - accuracy: 0.9694 - val_loss: 0.1953 - val_accuracy: 0.9414\n",
            "Epoch 26/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0808 - accuracy: 0.9704 - val_loss: 0.2044 - val_accuracy: 0.9411\n",
            "Epoch 27/30\n",
            "76/76 [==============================] - 24s 323ms/step - loss: 0.0780 - accuracy: 0.9714 - val_loss: 0.2044 - val_accuracy: 0.9407\n",
            "Epoch 28/30\n",
            "76/76 [==============================] - 24s 310ms/step - loss: 0.0723 - accuracy: 0.9734 - val_loss: 0.2325 - val_accuracy: 0.9422\n",
            "Epoch 29/30\n",
            "76/76 [==============================] - 23s 309ms/step - loss: 0.0682 - accuracy: 0.9749 - val_loss: 0.2302 - val_accuracy: 0.9415\n",
            "Epoch 30/30\n",
            "76/76 [==============================] - 23s 309ms/step - loss: 0.0654 - accuracy: 0.9756 - val_loss: 0.2399 - val_accuracy: 0.9356\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 1\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 1\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/fold_1/my_model_1/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_1/my_model_1/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_1/my_model_1/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c0416ce50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c0416ce50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c0416e7a0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c0416e7a0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c60524970> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c60524970> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c60526bc0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c60526bc0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c60559240> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c60559240> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "Evaluate the model on the test set after cross-validation\n",
            "9/9 [==============================] - 3s 81ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.670402 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.670402 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.47      0.38       194\n",
            "           1       0.97      0.95      0.96      3930\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.65      0.71      0.67      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.47      0.38       194\n",
            "           1       0.97      0.95      0.96      3930\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.65      0.71      0.67      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1283790   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 50, 100)              4279300   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 50, 400)              0         ['emb[0][0]',                 \n",
            " )                                                                   'embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " hsmm_bottom_1 (HSMMBottom)  (None, 2, 150)               2916906   ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 150)                  0         ['hsmm_bottom_1[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3  (None, 150)                  0         ['hsmm_bottom_1[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower_2 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " hsmm_tower_3 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_3[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack_1 (TFOpLambda)     (None, 2, 2)                 0         ['hsmm_tower_2[0][0]',        \n",
            "                                                                     'hsmm_tower_3[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims_1 (TFOpLamb  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_1 (TFOpLa  (None, 2, 1)                 0         ['tf.stack_1[0][0]',          \n",
            " mbda)                                                               'tf.expand_dims_1[0][0]']    \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_1 (TF  (None, 2)                    0         ['tf.linalg.matmul_1[0][0]']  \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20049410 (76.48 MB)\n",
            "Trainable params: 20049410 (76.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 2/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 2/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 2/5\n",
            "Epoch 1/30\n",
            "76/76 [==============================] - 37s 350ms/step - loss: 0.2686 - accuracy: 0.9275 - val_loss: 0.2845 - val_accuracy: 0.9323\n",
            "Epoch 2/30\n",
            "76/76 [==============================] - 25s 329ms/step - loss: 0.2231 - accuracy: 0.9296 - val_loss: 0.1910 - val_accuracy: 0.9323\n",
            "Epoch 3/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.2038 - accuracy: 0.9306 - val_loss: 0.2032 - val_accuracy: 0.9358\n",
            "Epoch 4/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.1909 - accuracy: 0.9343 - val_loss: 0.1789 - val_accuracy: 0.9360\n",
            "Epoch 5/30\n",
            "76/76 [==============================] - 24s 314ms/step - loss: 0.1817 - accuracy: 0.9360 - val_loss: 0.2192 - val_accuracy: 0.9303\n",
            "Epoch 6/30\n",
            "76/76 [==============================] - 25s 327ms/step - loss: 0.1754 - accuracy: 0.9370 - val_loss: 0.1656 - val_accuracy: 0.9414\n",
            "Epoch 7/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1701 - accuracy: 0.9398 - val_loss: 0.1694 - val_accuracy: 0.9361\n",
            "Epoch 8/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1649 - accuracy: 0.9410 - val_loss: 0.1646 - val_accuracy: 0.9407\n",
            "Epoch 9/30\n",
            "76/76 [==============================] - 24s 314ms/step - loss: 0.1602 - accuracy: 0.9417 - val_loss: 0.1522 - val_accuracy: 0.9450\n",
            "Epoch 10/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1555 - accuracy: 0.9436 - val_loss: 0.1729 - val_accuracy: 0.9416\n",
            "Epoch 11/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1518 - accuracy: 0.9444 - val_loss: 0.1637 - val_accuracy: 0.9440\n",
            "Epoch 12/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1481 - accuracy: 0.9462 - val_loss: 0.1943 - val_accuracy: 0.9338\n",
            "Epoch 13/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1455 - accuracy: 0.9478 - val_loss: 0.1479 - val_accuracy: 0.9477\n",
            "Epoch 14/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1404 - accuracy: 0.9489 - val_loss: 0.1515 - val_accuracy: 0.9447\n",
            "Epoch 15/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1350 - accuracy: 0.9511 - val_loss: 0.1518 - val_accuracy: 0.9477\n",
            "Epoch 16/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1303 - accuracy: 0.9527 - val_loss: 0.1513 - val_accuracy: 0.9468\n",
            "Epoch 17/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1265 - accuracy: 0.9547 - val_loss: 0.1580 - val_accuracy: 0.9430\n",
            "Epoch 18/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1221 - accuracy: 0.9556 - val_loss: 0.1618 - val_accuracy: 0.9475\n",
            "Epoch 19/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1181 - accuracy: 0.9574 - val_loss: 0.1528 - val_accuracy: 0.9489\n",
            "Epoch 20/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1126 - accuracy: 0.9594 - val_loss: 0.1516 - val_accuracy: 0.9475\n",
            "Epoch 21/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1088 - accuracy: 0.9609 - val_loss: 0.1695 - val_accuracy: 0.9501\n",
            "Epoch 22/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1055 - accuracy: 0.9615 - val_loss: 0.1648 - val_accuracy: 0.9473\n",
            "Epoch 23/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1006 - accuracy: 0.9637 - val_loss: 0.2009 - val_accuracy: 0.9320\n",
            "Epoch 24/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0959 - accuracy: 0.9655 - val_loss: 0.1875 - val_accuracy: 0.9471\n",
            "Epoch 25/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.0926 - accuracy: 0.9665 - val_loss: 0.1771 - val_accuracy: 0.9421\n",
            "Epoch 26/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0873 - accuracy: 0.9680 - val_loss: 0.1909 - val_accuracy: 0.9415\n",
            "Epoch 27/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0820 - accuracy: 0.9695 - val_loss: 0.1989 - val_accuracy: 0.9427\n",
            "Epoch 28/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.0797 - accuracy: 0.9711 - val_loss: 0.2698 - val_accuracy: 0.9206\n",
            "Epoch 29/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0761 - accuracy: 0.9725 - val_loss: 0.1975 - val_accuracy: 0.9450\n",
            "Epoch 30/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0718 - accuracy: 0.9741 - val_loss: 0.2355 - val_accuracy: 0.9485\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 2\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 2\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 2\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/fold_2/my_model_2/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_2/my_model_2/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_2/my_model_2/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_2/my_model_2/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7be0d82e90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7be0d82e90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7be0d82e90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7be0d809d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7be0d809d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7be0d809d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bedc21030> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bedc21030> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bedc21030> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bedc21150> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bedc21150> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bedc21150> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef97e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef97e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef97e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "Evaluate the model on the test set after cross-validation\n",
            "9/9 [==============================] - 2s 81ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.654994 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.654994 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.654994 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.52      0.35       142\n",
            "           1       0.98      0.95      0.96      3982\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.73      0.65      4124\n",
            "weighted avg       0.96      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.52      0.35       142\n",
            "           1       0.98      0.95      0.96      3982\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.73      0.65      4124\n",
            "weighted avg       0.96      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.52      0.35       142\n",
            "           1       0.98      0.95      0.96      3982\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.73      0.65      4124\n",
            "weighted avg       0.96      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1283790   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 50, 100)              4279300   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 50, 400)              0         ['emb[0][0]',                 \n",
            " )                                                                   'embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " hsmm_bottom_2 (HSMMBottom)  (None, 2, 150)               2916906   ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4  (None, 150)                  0         ['hsmm_bottom_2[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5  (None, 150)                  0         ['hsmm_bottom_2[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower_4 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_4[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " hsmm_tower_5 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_5[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack_2 (TFOpLambda)     (None, 2, 2)                 0         ['hsmm_tower_4[0][0]',        \n",
            "                                                                     'hsmm_tower_5[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims_2 (TFOpLamb  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_2 (TFOpLa  (None, 2, 1)                 0         ['tf.stack_2[0][0]',          \n",
            " mbda)                                                               'tf.expand_dims_2[0][0]']    \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_2 (TF  (None, 2)                    0         ['tf.linalg.matmul_2[0][0]']  \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20049410 (76.48 MB)\n",
            "Trainable params: 20049410 (76.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 3/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 3/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 3/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 3/5\n",
            "Epoch 1/30\n",
            "76/76 [==============================] - 37s 338ms/step - loss: 0.2740 - accuracy: 0.9186 - val_loss: 0.2249 - val_accuracy: 0.9306\n",
            "Epoch 2/30\n",
            "76/76 [==============================] - 24s 317ms/step - loss: 0.2177 - accuracy: 0.9300 - val_loss: 0.1979 - val_accuracy: 0.9306\n",
            "Epoch 3/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.2010 - accuracy: 0.9323 - val_loss: 0.2010 - val_accuracy: 0.9314\n",
            "Epoch 4/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.1859 - accuracy: 0.9359 - val_loss: 0.1837 - val_accuracy: 0.9351\n",
            "Epoch 5/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1770 - accuracy: 0.9381 - val_loss: 0.1801 - val_accuracy: 0.9382\n",
            "Epoch 6/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1703 - accuracy: 0.9401 - val_loss: 0.1708 - val_accuracy: 0.9394\n",
            "Epoch 7/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1660 - accuracy: 0.9419 - val_loss: 0.1726 - val_accuracy: 0.9410\n",
            "Epoch 8/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1605 - accuracy: 0.9435 - val_loss: 0.1982 - val_accuracy: 0.9304\n",
            "Epoch 9/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1549 - accuracy: 0.9449 - val_loss: 0.1704 - val_accuracy: 0.9415\n",
            "Epoch 10/30\n",
            "76/76 [==============================] - 24s 314ms/step - loss: 0.1521 - accuracy: 0.9464 - val_loss: 0.1656 - val_accuracy: 0.9411\n",
            "Epoch 11/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1478 - accuracy: 0.9476 - val_loss: 0.1998 - val_accuracy: 0.9225\n",
            "Epoch 12/30\n",
            "76/76 [==============================] - 24s 314ms/step - loss: 0.1428 - accuracy: 0.9490 - val_loss: 0.1622 - val_accuracy: 0.9424\n",
            "Epoch 13/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1399 - accuracy: 0.9499 - val_loss: 0.1932 - val_accuracy: 0.9256\n",
            "Epoch 14/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1358 - accuracy: 0.9510 - val_loss: 0.1760 - val_accuracy: 0.9385\n",
            "Epoch 15/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1332 - accuracy: 0.9527 - val_loss: 0.1669 - val_accuracy: 0.9423\n",
            "Epoch 16/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1275 - accuracy: 0.9536 - val_loss: 0.1667 - val_accuracy: 0.9424\n",
            "Epoch 17/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1242 - accuracy: 0.9548 - val_loss: 0.1821 - val_accuracy: 0.9414\n",
            "Epoch 18/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1196 - accuracy: 0.9567 - val_loss: 0.1716 - val_accuracy: 0.9429\n",
            "Epoch 19/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1172 - accuracy: 0.9589 - val_loss: 0.1865 - val_accuracy: 0.9303\n",
            "Epoch 20/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1123 - accuracy: 0.9603 - val_loss: 0.1806 - val_accuracy: 0.9343\n",
            "Epoch 21/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1077 - accuracy: 0.9618 - val_loss: 0.1813 - val_accuracy: 0.9418\n",
            "Epoch 22/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1039 - accuracy: 0.9629 - val_loss: 0.1809 - val_accuracy: 0.9390\n",
            "Epoch 23/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1005 - accuracy: 0.9633 - val_loss: 0.1792 - val_accuracy: 0.9406\n",
            "Epoch 24/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0949 - accuracy: 0.9665 - val_loss: 0.1985 - val_accuracy: 0.9392\n",
            "Epoch 25/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0940 - accuracy: 0.9664 - val_loss: 0.2058 - val_accuracy: 0.9323\n",
            "Epoch 26/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0875 - accuracy: 0.9692 - val_loss: 0.2095 - val_accuracy: 0.9400\n",
            "Epoch 27/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.0843 - accuracy: 0.9697 - val_loss: 0.2268 - val_accuracy: 0.9331\n",
            "Epoch 28/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0827 - accuracy: 0.9707 - val_loss: 0.2051 - val_accuracy: 0.9426\n",
            "Epoch 29/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.0769 - accuracy: 0.9722 - val_loss: 0.2276 - val_accuracy: 0.9360\n",
            "Epoch 30/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 0.2322 - val_accuracy: 0.9417\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 3\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 3\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 3\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 3\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/fold_3/my_model_3/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_3/my_model_3/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_3/my_model_3/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_3/my_model_3/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_3/my_model_3/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed681c90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed681c90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed681c90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed681c90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed680460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed680460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed680460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed680460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed65a8f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed65a8f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed65a8f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed65a8f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed6598d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed6598d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed6598d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bed6598d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d791bfedae0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d791bfedae0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d791bfedae0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d791bfedae0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "Evaluate the model on the test set after cross-validation\n",
            "9/9 [==============================] - 2s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.645562 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.645562 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.645562 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.645562 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.47      0.33       152\n",
            "           1       0.98      0.95      0.96      3972\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.71      0.65      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.47      0.33       152\n",
            "           1       0.98      0.95      0.96      3972\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.71      0.65      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.47      0.33       152\n",
            "           1       0.98      0.95      0.96      3972\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.71      0.65      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.47      0.33       152\n",
            "           1       0.98      0.95      0.96      3972\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.62      0.71      0.65      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1283790   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)     (None, 50, 100)              4279300   ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 50, 400)              0         ['emb[0][0]',                 \n",
            " )                                                                   'embedding_3[0][0]']         \n",
            "                                                                                                  \n",
            " hsmm_bottom_3 (HSMMBottom)  (None, 2, 150)               2916906   ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6  (None, 150)                  0         ['hsmm_bottom_3[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_7  (None, 150)                  0         ['hsmm_bottom_3[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower_6 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_6[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " hsmm_tower_7 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_7[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack_3 (TFOpLambda)     (None, 2, 2)                 0         ['hsmm_tower_6[0][0]',        \n",
            "                                                                     'hsmm_tower_7[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims_3 (TFOpLamb  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_3 (TFOpLa  (None, 2, 1)                 0         ['tf.stack_3[0][0]',          \n",
            " mbda)                                                               'tf.expand_dims_3[0][0]']    \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_3 (TF  (None, 2)                    0         ['tf.linalg.matmul_3[0][0]']  \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20049410 (76.48 MB)\n",
            "Trainable params: 20049410 (76.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 4/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 4/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 4/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 4/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 4/5\n",
            "Epoch 1/30\n",
            "76/76 [==============================] - 37s 336ms/step - loss: 0.2677 - accuracy: 0.9223 - val_loss: 0.2328 - val_accuracy: 0.9265\n",
            "Epoch 2/30\n",
            "76/76 [==============================] - 25s 330ms/step - loss: 0.2180 - accuracy: 0.9310 - val_loss: 0.2135 - val_accuracy: 0.9265\n",
            "Epoch 3/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.1990 - accuracy: 0.9321 - val_loss: 0.1989 - val_accuracy: 0.9298\n",
            "Epoch 4/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.1882 - accuracy: 0.9356 - val_loss: 0.1880 - val_accuracy: 0.9318\n",
            "Epoch 5/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1793 - accuracy: 0.9381 - val_loss: 0.1873 - val_accuracy: 0.9329\n",
            "Epoch 6/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1707 - accuracy: 0.9394 - val_loss: 0.1957 - val_accuracy: 0.9351\n",
            "Epoch 7/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1640 - accuracy: 0.9421 - val_loss: 0.1765 - val_accuracy: 0.9359\n",
            "Epoch 8/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1609 - accuracy: 0.9430 - val_loss: 0.1758 - val_accuracy: 0.9363\n",
            "Epoch 9/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1563 - accuracy: 0.9439 - val_loss: 0.1727 - val_accuracy: 0.9367\n",
            "Epoch 10/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1505 - accuracy: 0.9471 - val_loss: 0.1834 - val_accuracy: 0.9299\n",
            "Epoch 11/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1473 - accuracy: 0.9470 - val_loss: 0.1752 - val_accuracy: 0.9383\n",
            "Epoch 12/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1445 - accuracy: 0.9479 - val_loss: 0.1859 - val_accuracy: 0.9317\n",
            "Epoch 13/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1393 - accuracy: 0.9515 - val_loss: 0.1727 - val_accuracy: 0.9404\n",
            "Epoch 14/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1365 - accuracy: 0.9515 - val_loss: 0.1663 - val_accuracy: 0.9396\n",
            "Epoch 15/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1297 - accuracy: 0.9543 - val_loss: 0.1748 - val_accuracy: 0.9350\n",
            "Epoch 16/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1288 - accuracy: 0.9538 - val_loss: 0.1795 - val_accuracy: 0.9336\n",
            "Epoch 17/30\n",
            "76/76 [==============================] - 24s 314ms/step - loss: 0.1215 - accuracy: 0.9570 - val_loss: 0.1843 - val_accuracy: 0.9329\n",
            "Epoch 18/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1192 - accuracy: 0.9587 - val_loss: 0.1820 - val_accuracy: 0.9379\n",
            "Epoch 19/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1140 - accuracy: 0.9601 - val_loss: 0.1779 - val_accuracy: 0.9371\n",
            "Epoch 20/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1090 - accuracy: 0.9626 - val_loss: 0.1925 - val_accuracy: 0.9325\n",
            "Epoch 21/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1048 - accuracy: 0.9630 - val_loss: 0.1972 - val_accuracy: 0.9279\n",
            "Epoch 22/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1012 - accuracy: 0.9650 - val_loss: 0.1982 - val_accuracy: 0.9327\n",
            "Epoch 23/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.0970 - accuracy: 0.9664 - val_loss: 0.2298 - val_accuracy: 0.9212\n",
            "Epoch 24/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0924 - accuracy: 0.9676 - val_loss: 0.2045 - val_accuracy: 0.9387\n",
            "Epoch 25/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0897 - accuracy: 0.9701 - val_loss: 0.1945 - val_accuracy: 0.9388\n",
            "Epoch 26/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0844 - accuracy: 0.9712 - val_loss: 0.2041 - val_accuracy: 0.9339\n",
            "Epoch 27/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0817 - accuracy: 0.9721 - val_loss: 0.2158 - val_accuracy: 0.9406\n",
            "Epoch 28/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.0778 - accuracy: 0.9722 - val_loss: 0.2185 - val_accuracy: 0.9388\n",
            "Epoch 29/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.2186 - val_accuracy: 0.9380\n",
            "Epoch 30/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0708 - accuracy: 0.9740 - val_loss: 0.2075 - val_accuracy: 0.9367\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 4\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 4\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 4\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 4\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 4\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/fold_4/my_model_4/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_4/my_model_4/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_4/my_model_4/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_4/my_model_4/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_4/my_model_4/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_4/my_model_4/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1acb20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1acb20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1acb20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1acb20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1acb20> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1de500> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1de500> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1de500> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1de500> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec1de500> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f4490> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f4490> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f4490> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f4490> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f4490> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f66e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f66e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f66e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f66e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c681f66e0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c68163370> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c68163370> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c68163370> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c68163370> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c68163370> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "Evaluate the model on the test set after cross-validation\n",
            "9/9 [==============================] - 3s 86ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.635605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.635605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.635605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.635605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.635605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.46      0.31       147\n",
            "           1       0.98      0.94      0.96      3977\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.61      0.70      0.64      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.46      0.31       147\n",
            "           1       0.98      0.94      0.96      3977\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.61      0.70      0.64      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.46      0.31       147\n",
            "           1       0.98      0.94      0.96      3977\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.61      0.70      0.64      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.46      0.31       147\n",
            "           1       0.98      0.94      0.96      3977\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.61      0.70      0.64      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.46      0.31       147\n",
            "           1       0.98      0.94      0.96      3977\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.61      0.70      0.64      4124\n",
            "weighted avg       0.95      0.93      0.94      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1283790   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, 50, 100)              4279300   ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 50, 400)              0         ['emb[0][0]',                 \n",
            " )                                                                   'embedding_4[0][0]']         \n",
            "                                                                                                  \n",
            " hsmm_bottom_4 (HSMMBottom)  (None, 2, 150)               2916906   ['concatenate_4[0][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_8  (None, 150)                  0         ['hsmm_bottom_4[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_9  (None, 150)                  0         ['hsmm_bottom_4[0][0]']       \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower_8 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_8[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " hsmm_tower_9 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_9[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack_4 (TFOpLambda)     (None, 2, 2)                 0         ['hsmm_tower_8[0][0]',        \n",
            "                                                                     'hsmm_tower_9[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims_4 (TFOpLamb  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_4 (TFOpLa  (None, 2, 1)                 0         ['tf.stack_4[0][0]',          \n",
            " mbda)                                                               'tf.expand_dims_4[0][0]']    \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_4 (TF  (None, 2)                    0         ['tf.linalg.matmul_4[0][0]']  \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20049410 (76.48 MB)\n",
            "Trainable params: 20049410 (76.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 5/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 5/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 5/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 5/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 5/5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training fold 5/5\n",
            "Epoch 1/30\n",
            "76/76 [==============================] - 38s 339ms/step - loss: 0.2664 - accuracy: 0.9280 - val_loss: 0.2328 - val_accuracy: 0.9288\n",
            "Epoch 2/30\n",
            "76/76 [==============================] - 24s 318ms/step - loss: 0.2166 - accuracy: 0.9305 - val_loss: 0.2509 - val_accuracy: 0.9288\n",
            "Epoch 3/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1986 - accuracy: 0.9320 - val_loss: 0.1838 - val_accuracy: 0.9334\n",
            "Epoch 4/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1887 - accuracy: 0.9351 - val_loss: 0.1818 - val_accuracy: 0.9334\n",
            "Epoch 5/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1773 - accuracy: 0.9379 - val_loss: 0.1823 - val_accuracy: 0.9342\n",
            "Epoch 6/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1732 - accuracy: 0.9392 - val_loss: 0.1715 - val_accuracy: 0.9361\n",
            "Epoch 7/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1654 - accuracy: 0.9411 - val_loss: 0.2295 - val_accuracy: 0.9165\n",
            "Epoch 8/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.1596 - accuracy: 0.9430 - val_loss: 0.1897 - val_accuracy: 0.9341\n",
            "Epoch 9/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1552 - accuracy: 0.9450 - val_loss: 0.1688 - val_accuracy: 0.9374\n",
            "Epoch 10/30\n",
            "76/76 [==============================] - 24s 315ms/step - loss: 0.1508 - accuracy: 0.9456 - val_loss: 0.1682 - val_accuracy: 0.9389\n",
            "Epoch 11/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1457 - accuracy: 0.9483 - val_loss: 0.1711 - val_accuracy: 0.9392\n",
            "Epoch 12/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1433 - accuracy: 0.9491 - val_loss: 0.1667 - val_accuracy: 0.9377\n",
            "Epoch 13/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1361 - accuracy: 0.9517 - val_loss: 0.1792 - val_accuracy: 0.9397\n",
            "Epoch 14/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1336 - accuracy: 0.9518 - val_loss: 0.1628 - val_accuracy: 0.9390\n",
            "Epoch 15/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1312 - accuracy: 0.9534 - val_loss: 0.1633 - val_accuracy: 0.9405\n",
            "Epoch 16/30\n",
            "76/76 [==============================] - 25s 325ms/step - loss: 0.1260 - accuracy: 0.9545 - val_loss: 0.1755 - val_accuracy: 0.9405\n",
            "Epoch 17/30\n",
            "76/76 [==============================] - 24s 314ms/step - loss: 0.1211 - accuracy: 0.9570 - val_loss: 0.1689 - val_accuracy: 0.9367\n",
            "Epoch 18/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1187 - accuracy: 0.9573 - val_loss: 0.1635 - val_accuracy: 0.9406\n",
            "Epoch 19/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1129 - accuracy: 0.9597 - val_loss: 0.1783 - val_accuracy: 0.9380\n",
            "Epoch 20/30\n",
            "76/76 [==============================] - 25s 326ms/step - loss: 0.1104 - accuracy: 0.9603 - val_loss: 0.1716 - val_accuracy: 0.9408\n",
            "Epoch 21/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.1039 - accuracy: 0.9628 - val_loss: 0.1737 - val_accuracy: 0.9379\n",
            "Epoch 22/30\n",
            "76/76 [==============================] - 24s 313ms/step - loss: 0.1010 - accuracy: 0.9636 - val_loss: 0.2031 - val_accuracy: 0.9396\n",
            "Epoch 23/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0977 - accuracy: 0.9650 - val_loss: 0.1804 - val_accuracy: 0.9400\n",
            "Epoch 24/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.0915 - accuracy: 0.9674 - val_loss: 0.1965 - val_accuracy: 0.9398\n",
            "Epoch 25/30\n",
            "76/76 [==============================] - 25s 324ms/step - loss: 0.0891 - accuracy: 0.9686 - val_loss: 0.2071 - val_accuracy: 0.9326\n",
            "Epoch 26/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0843 - accuracy: 0.9697 - val_loss: 0.2149 - val_accuracy: 0.9403\n",
            "Epoch 27/30\n",
            "76/76 [==============================] - 24s 311ms/step - loss: 0.0814 - accuracy: 0.9713 - val_loss: 0.2055 - val_accuracy: 0.9389\n",
            "Epoch 28/30\n",
            "76/76 [==============================] - 24s 312ms/step - loss: 0.0755 - accuracy: 0.9732 - val_loss: 0.2124 - val_accuracy: 0.9395\n",
            "Epoch 29/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0738 - accuracy: 0.9737 - val_loss: 0.2224 - val_accuracy: 0.9377\n",
            "Epoch 30/30\n",
            "76/76 [==============================] - 25s 323ms/step - loss: 0.0680 - accuracy: 0.9758 - val_loss: 0.2352 - val_accuracy: 0.9362\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 5\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture for fold 5\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/fold_5/my_model_5/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c7c1556f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c7c1556f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c7c1556f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c7c1556f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c7c1556f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7c7c1556f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964400> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964400> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964400> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964400> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964400> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964400> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964fa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964fa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964fa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964fa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964fa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7bec964fa0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8910> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8910> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8910> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8910> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8910> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8910> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8700> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8700> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8700> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8700> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8700> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x7d7becef8700> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "Evaluate the model on the test set after cross-validation\n",
            "9/9 [==============================] - 2s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 30:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.663287 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.663287 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.663287 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.663287 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.663287 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.663287 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37       200\n",
            "           1       0.97      0.95      0.96      3924\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.64      0.70      0.66      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37       200\n",
            "           1       0.97      0.95      0.96      3924\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.64      0.70      0.66      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37       200\n",
            "           1       0.97      0.95      0.96      3924\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.64      0.70      0.66      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37       200\n",
            "           1       0.97      0.95      0.96      3924\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.64      0.70      0.66      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37       200\n",
            "           1       0.97      0.95      0.96      3924\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.64      0.70      0.66      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.45      0.37       200\n",
            "           1       0.97      0.95      0.96      3924\n",
            "\n",
            "    accuracy                           0.93      4124\n",
            "   macro avg       0.64      0.70      0.66      4124\n",
            "weighted avg       0.94      0.93      0.93      4124\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS'\n",
        "output_path = ROOT_PROJECT+'/SKS/outputs/davidson'\n",
        "if not os.path.exists(output_path):\n",
        "  os.makedirs(output_path)\n",
        "!python DNN/train.py -d data/davidson/train_data.csv --trial data/davidson/test_data.csv -s data/sentiment_datasets/train_E6oV3lV.csv --word_list data/word_list/word_all.txt --emb data/glove.6B.300d.txt -o outputs/davidson -b 512 --epochs 30 --lr 0.002 --maxlen 50 -t HHMM_transformer --cross_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIc4nQAYistk",
        "outputId": "978e1253-f0fd-49ac-c296-e7de40a02165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1fsGCaPYe4CP4WYC0FCHd3HuqJ6TOUA1j/Project_CS221/SKS\n",
            "2023-12-21 05:50:15.237214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-21 05:50:15.237260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-21 05:50:15.238705: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-21 05:50:15.245762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-21 05:50:16.261925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-21 05:50:19.625700: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:50:20.176731: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:50:20.177144: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:50:20.411219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:50:20.411644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:50:20.411953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:50:20.412137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\u001b[94m[INFO]\u001b[0m (utils) Arguments:\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   batch_size: 512\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   command: DNN/train.py -d data/davidson/train_data.csv --trial data/davidson/test_data.csv -s data/sentiment_datasets/train_E6oV3lV.csv --word_list data/word_list/word_all.txt --emb data/glove.6B.300d.txt -o outputs/davidson/final -b 512 --epochs 50 --lr 0.002 --maxlen 50 -t HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   cross_validation: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   data_path: data/davidson/train_data.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   dropout_prob: 0.1\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   emb_dim: 300\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   emb_path: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   epochs: 50\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   humor_data_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   learn_rate: 0.002\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   loss: ce\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   maxlen: 50\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   model_retrain_cross_validation_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   model_type: HHMM_transformer\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   non_gate: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   out_dir_path: outputs/davidson/final\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   retrain_cross_validation: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   sarcasm_data_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   sentiment_data_path: data/sentiment_datasets/train_E6oV3lV.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   trial_data_path: data/davidson/test_data.csv\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   vocab_path: None\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   word_list_path: data/word_list/word_all.txt\n",
            "\u001b[94m[INFO]\u001b[0m (utils)   word_norm: 1\n",
            "test_task size>>> 4124\n",
            "test_task size>>> 4444\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) Creating vocabulary.........\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader)   622876 total words, 42793 unique words\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader)   Vocab size: 42793\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) <unk> hit rate: 0.00%\n",
            "\u001b[94m[INFO]\u001b[0m (data_reader) <unk> hit rate: 0.01%\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Statistics:\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   train_x shape: (48458, 50)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   test_x shape:  (4444, 50)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   train_chars shape: (48458, 300)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   test_chars shape:  (4444, 300)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   train_y shape: (48458, 2)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__)   test_y shape:  (4444, 2)\n",
            "\u001b[94m[INFO]\u001b[0m (models) Building a HHMM_transformer\n",
            "2023-12-21 05:51:24.818398: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.818750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.818988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.819527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.819771: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.819981: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.820246: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.820479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-21 05:51:24.820641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " sequence_input (InputLayer  [(None, 50)]                 0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 50)]                 0         []                            \n",
            "                                                                                                  \n",
            " emb (Embedding)             (None, 50, 300)              1283790   ['sequence_input[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 50, 100)              4279300   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 50, 400)              0         ['emb[0][0]',                 \n",
            "                                                                     'embedding[0][0]']           \n",
            "                                                                                                  \n",
            " hsmm_bottom (HSMMBottom)    (None, 2, 150)               2916906   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 150)                  0         ['hsmm_bottom[0][0]']         \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  (None, 150)                  0         ['hsmm_bottom[0][0]']         \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " hsmm_tower (HSMMTower)      (None, 2)                    7652      ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " hsmm_tower_1 (HSMMTower)    (None, 2)                    7652      ['tf.__operators__.getitem_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " taskid_input (InputLayer)   [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " tf.stack (TFOpLambda)       (None, 2, 2)                 0         ['hsmm_tower[0][0]',          \n",
            "                                                                     'hsmm_tower_1[0][0]']        \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda  (None, 2, 1)                 0         ['taskid_input[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLamb  (None, 2, 1)                 0         ['tf.stack[0][0]',            \n",
            " da)                                                                 'tf.expand_dims[0][0]']      \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze (TFOp  (None, 2)                    0         ['tf.linalg.matmul[0][0]']    \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20049410 (76.48 MB)\n",
            "Trainable params: 20049410 (76.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "\u001b[94m[INFO]\u001b[0m (models) Initializing lookup table\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) Loading embeddings from: data/glove.6B.300d.txt\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader)   #vectors: 400001, #dimensions: 300\n",
            "\u001b[94m[INFO]\u001b[0m (w2vEmbReader) 21933/42793 word vectors initialized (hit rate: 51.25%)\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Saving model architecture\n",
            "2023-12-21 05:52:30.029723: I external/local_xla/xla/service/service.cc:168] XLA service 0x79b8e82006c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-21 05:52:30.029767: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-12-21 05:52:30.089150: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-12-21 05:52:30.200458: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1703137950.351383    2628 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "95/95 [==============================] - 45s 305ms/step - loss: 0.2668 - accuracy: 0.9213 - val_loss: 0.2516 - val_accuracy: 0.9273\n",
            "9/9 [==============================] - 2s 77ms/step\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 0, train: 45s, evaluation: 14s, total_time: 60s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2668, metric: 0.9213\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.930650 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.482040 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.964079 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.798145 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 299ms/step - loss: 0.2123 - accuracy: 0.9301 - val_loss: 0.2287 - val_accuracy: 0.9273\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 1, train: 41s, evaluation: 12s, total_time: 113s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.2123, metric: 0.9301\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.930650 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.482040 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.964079 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.798145 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 296ms/step - loss: 0.1977 - accuracy: 0.9326 - val_loss: 0.1985 - val_accuracy: 0.9275\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 2, train: 41s, evaluation: 11s, total_time: 166s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1977, metric: 0.9326\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.930650 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.482040 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.964079 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.797758 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 302ms/step - loss: 0.1806 - accuracy: 0.9364 - val_loss: 0.1960 - val_accuracy: 0.9269\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 3, train: 41s, evaluation: 13s, total_time: 220s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1806, metric: 0.9364\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.929680 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.501619 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.959920 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.798903 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 307ms/step - loss: 0.1747 - accuracy: 0.9383 - val_loss: 0.1934 - val_accuracy: 0.9264\n",
            "9/9 [==============================] - 1s 81ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 4, train: 41s, evaluation: 13s, total_time: 274s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1747, metric: 0.9383\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.928710 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.516278 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.956437 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.797666 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 308ms/step - loss: 0.1670 - accuracy: 0.9410 - val_loss: 0.1890 - val_accuracy: 0.9282\n",
            "9/9 [==============================] - 1s 85ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 5, train: 29s, evaluation: 13s, total_time: 317s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1670, metric: 0.9410\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.929922 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.581402 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.949465 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.812533 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 301ms/step - loss: 0.1607 - accuracy: 0.9419 - val_loss: 0.1904 - val_accuracy: 0.9300\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 6, train: 41s, evaluation: 13s, total_time: 372s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1607, metric: 0.9419\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.931620 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.603065 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.949133 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.818904 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 308ms/step - loss: 0.1561 - accuracy: 0.9435 - val_loss: 0.1961 - val_accuracy: 0.9305\n",
            "9/9 [==============================] - 1s 85ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 7, train: 29s, evaluation: 11s, total_time: 413s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1561, metric: 0.9435\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932105 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.642328 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.944595 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.826368 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 306ms/step - loss: 0.1525 - accuracy: 0.9454 - val_loss: 0.1875 - val_accuracy: 0.9307\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 8, train: 41s, evaluation: 2s, total_time: 457s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1525, metric: 0.9454\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.931862 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.619078 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.947347 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.822110 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 302ms/step - loss: 0.1479 - accuracy: 0.9470 - val_loss: 0.1857 - val_accuracy: 0.9289\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 9, train: 41s, evaluation: 12s, total_time: 510s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1479, metric: 0.9470\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.928952 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.660627 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.937367 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.827657 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 305ms/step - loss: 0.1437 - accuracy: 0.9475 - val_loss: 0.2197 - val_accuracy: 0.9298\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 10, train: 41s, evaluation: 11s, total_time: 563s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1437, metric: 0.9475\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.930407 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.666201 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.938833 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.830687 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 302ms/step - loss: 0.1396 - accuracy: 0.9492 - val_loss: 0.1914 - val_accuracy: 0.9320\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 11, train: 41s, evaluation: 2s, total_time: 606s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1396, metric: 0.9492\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.933560 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.616291 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.950311 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.824943 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 307ms/step - loss: 0.1363 - accuracy: 0.9498 - val_loss: 0.1849 - val_accuracy: 0.9334\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 12, train: 41s, evaluation: 1s, total_time: 649s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1363, metric: 0.9498\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932832 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.640366 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.945965 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.828731 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 301ms/step - loss: 0.1325 - accuracy: 0.9507 - val_loss: 0.2072 - val_accuracy: 0.9325\n",
            "9/9 [==============================] - 1s 84ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 13, train: 28s, evaluation: 2s, total_time: 680s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1325, metric: 0.9507\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.933317 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.564704 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.956920 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.818131 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 301ms/step - loss: 0.1289 - accuracy: 0.9532 - val_loss: 0.1865 - val_accuracy: 0.9332\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 14, train: 41s, evaluation: 2s, total_time: 724s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1289, metric: 0.9532\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932832 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.592312 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.952438 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.822169 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 300ms/step - loss: 0.1248 - accuracy: 0.9537 - val_loss: 0.1977 - val_accuracy: 0.9325\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 15, train: 41s, evaluation: 2s, total_time: 767s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1248, metric: 0.9537\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.933075 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.592688 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.952758 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.821553 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 309ms/step - loss: 0.1209 - accuracy: 0.9550 - val_loss: 0.1980 - val_accuracy: 0.9354\n",
            "9/9 [==============================] - 1s 83ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 16, train: 29s, evaluation: 1s, total_time: 798s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1209, metric: 0.9550\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.934772 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.640638 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.948884 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.834388 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 303ms/step - loss: 0.1161 - accuracy: 0.9574 - val_loss: 0.2136 - val_accuracy: 0.9334\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 17, train: 28s, evaluation: 2s, total_time: 829s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1161, metric: 0.9574\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932832 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.645344 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.945298 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.832358 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 303ms/step - loss: 0.1126 - accuracy: 0.9592 - val_loss: 0.2032 - val_accuracy: 0.9298\n",
            "9/9 [==============================] - 1s 82ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 18, train: 28s, evaluation: 13s, total_time: 871s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1126, metric: 0.9592\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.927740 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.674220 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.933733 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.832079 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 301ms/step - loss: 0.1086 - accuracy: 0.9602 - val_loss: 0.1926 - val_accuracy: 0.9345\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 19, train: 41s, evaluation: 2s, total_time: 914s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1086, metric: 0.9602\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932832 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.650180 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.944651 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.834462 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 306ms/step - loss: 0.1047 - accuracy: 0.9620 - val_loss: 0.2134 - val_accuracy: 0.9224\n",
            "9/9 [==============================] - 1s 83ms/step\n",
            "INFO:tensorflow:Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[INFO]\u001b[0m (tensorflow) Assets written to: outputs/davidson/final/best_model/assets\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a6e60> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba105a5000> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79ba1055f460> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb9896f0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[WARNING]\u001b[0m (absl) <my_layers.MultiHeadAttention object at 0x79b9cb98bd30> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 'my_layers.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 20, train: 29s, evaluation: 12s, total_time: 956s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1047, metric: 0.9620\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.920466 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.706126 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.918640 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.829595 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 306ms/step - loss: 0.1019 - accuracy: 0.9621 - val_loss: 0.2199 - val_accuracy: 0.9350\n",
            "9/9 [==============================] - 1s 84ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 21, train: 29s, evaluation: 1s, total_time: 987s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.1019, metric: 0.9621\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.933560 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.666799 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.943536 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.839005 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 299ms/step - loss: 0.0956 - accuracy: 0.9648 - val_loss: 0.2073 - val_accuracy: 0.9361\n",
            "9/9 [==============================] - 1s 78ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 22, train: 41s, evaluation: 2s, total_time: 1030s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0956, metric: 0.9648\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.933560 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.671090 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.942964 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.841381 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 298ms/step - loss: 0.0929 - accuracy: 0.9652 - val_loss: 0.2500 - val_accuracy: 0.9347\n",
            "9/9 [==============================] - 1s 85ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 23, train: 28s, evaluation: 2s, total_time: 1061s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0929, metric: 0.9652\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932590 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.674648 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.941019 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.840231 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 308ms/step - loss: 0.0900 - accuracy: 0.9674 - val_loss: 0.2199 - val_accuracy: 0.9318\n",
            "9/9 [==============================] - 1s 81ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 24, train: 29s, evaluation: 2s, total_time: 1092s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0900, metric: 0.9674\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.929195 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.697076 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.932924 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.840446 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 300ms/step - loss: 0.0848 - accuracy: 0.9693 - val_loss: 0.2255 - val_accuracy: 0.9374\n",
            "9/9 [==============================] - 1s 82ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 25, train: 28s, evaluation: 2s, total_time: 1123s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0848, metric: 0.9693\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.935015 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.662312 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.946346 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.842553 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 298ms/step - loss: 0.0813 - accuracy: 0.9701 - val_loss: 0.2161 - val_accuracy: 0.9298\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 26, train: 41s, evaluation: 2s, total_time: 1166s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0813, metric: 0.9701\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.925558 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.689944 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.928376 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.837898 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 305ms/step - loss: 0.0774 - accuracy: 0.9721 - val_loss: 0.2291 - val_accuracy: 0.9354\n",
            "9/9 [==============================] - 1s 81ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 27, train: 41s, evaluation: 2s, total_time: 1210s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0774, metric: 0.9721\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.932105 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.659503 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.942300 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.839289 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 300ms/step - loss: 0.0751 - accuracy: 0.9727 - val_loss: 0.2505 - val_accuracy: 0.9336\n",
            "9/9 [==============================] - 1s 78ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 28, train: 41s, evaluation: 2s, total_time: 1254s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0751, metric: 0.9727\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.929437 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.681257 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.935371 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.841359 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 300ms/step - loss: 0.0704 - accuracy: 0.9742 - val_loss: 0.2660 - val_accuracy: 0.9350\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 29, train: 41s, evaluation: 2s, total_time: 1297s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0704, metric: 0.9742\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.933075 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.645808 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.945605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.835599 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 304ms/step - loss: 0.0673 - accuracy: 0.9753 - val_loss: 0.2889 - val_accuracy: 0.9334\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 30, train: 41s, evaluation: 1s, total_time: 1339s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0673, metric: 0.9753\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.929195 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.690315 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.933812 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.843005 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 297ms/step - loss: 0.0637 - accuracy: 0.9762 - val_loss: 0.2540 - val_accuracy: 0.9298\n",
            "9/9 [==============================] - 1s 82ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 31, train: 28s, evaluation: 2s, total_time: 1370s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0637, metric: 0.9762\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.925315 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.689481 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.928071 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.838151 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 296ms/step - loss: 0.0626 - accuracy: 0.9771 - val_loss: 0.2816 - val_accuracy: 0.9343\n",
            "9/9 [==============================] - 1s 83ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 32, train: 28s, evaluation: 2s, total_time: 1401s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0626, metric: 0.9771\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.930165 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.621651 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.944414 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.830432 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 299ms/step - loss: 0.0576 - accuracy: 0.9781 - val_loss: 0.3098 - val_accuracy: 0.9129\n",
            "9/9 [==============================] - 1s 83ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 33, train: 28s, evaluation: 2s, total_time: 1431s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0576, metric: 0.9781\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.907129 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.688485 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.900991 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.818528 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 300ms/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.3500 - val_accuracy: 0.9298\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 34, train: 41s, evaluation: 2s, total_time: 1475s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0565, metric: 0.9788\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.925800 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.684840 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.929409 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.836876 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 302ms/step - loss: 0.0534 - accuracy: 0.9800 - val_loss: 0.3070 - val_accuracy: 0.9282\n",
            "9/9 [==============================] - 1s 85ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 35, train: 28s, evaluation: 2s, total_time: 1506s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0534, metric: 0.9800\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.923618 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.661503 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.929179 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.829846 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 29s 301ms/step - loss: 0.0519 - accuracy: 0.9806 - val_loss: 0.3058 - val_accuracy: 0.9323\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 36, train: 41s, evaluation: 2s, total_time: 1549s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0519, metric: 0.9806\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.928225 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.672614 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.934678 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.836792 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 299ms/step - loss: 0.0497 - accuracy: 0.9820 - val_loss: 0.3299 - val_accuracy: 0.9311\n",
            "9/9 [==============================] - 1s 82ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 37, train: 41s, evaluation: 2s, total_time: 1593s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0497, metric: 0.9820\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.926528 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.651911 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.934849 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.831986 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 297ms/step - loss: 0.0442 - accuracy: 0.9834 - val_loss: 0.3477 - val_accuracy: 0.9109\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 38, train: 41s, evaluation: 2s, total_time: 1636s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0442, metric: 0.9834\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.904947 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.672665 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.899731 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.811101 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 300ms/step - loss: 0.0418 - accuracy: 0.9844 - val_loss: 0.3846 - val_accuracy: 0.9305\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 39, train: 41s, evaluation: 1s, total_time: 1679s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0418, metric: 0.9844\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.928225 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.638472 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.939212 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.827567 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 296ms/step - loss: 0.0419 - accuracy: 0.9848 - val_loss: 0.4071 - val_accuracy: 0.9314\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 40, train: 41s, evaluation: 2s, total_time: 1722s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0419, metric: 0.9848\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.926285 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.677588 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.931092 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.837840 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 294ms/step - loss: 0.0403 - accuracy: 0.9850 - val_loss: 0.3806 - val_accuracy: 0.9269\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 41, train: 41s, evaluation: 2s, total_time: 1766s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0403, metric: 0.9850\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.921920 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.680964 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.924072 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.832582 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 294ms/step - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.4440 - val_accuracy: 0.9280\n",
            "9/9 [==============================] - 1s 83ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 42, train: 28s, evaluation: 2s, total_time: 1796s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0364, metric: 0.9871\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.923133 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.677572 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.926340 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.833219 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 298ms/step - loss: 0.0382 - accuracy: 0.9864 - val_loss: 0.3509 - val_accuracy: 0.9287\n",
            "9/9 [==============================] - 1s 78ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 43, train: 41s, evaluation: 2s, total_time: 1840s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0382, metric: 0.9864\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.923375 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.662344 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.928703 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.830913 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 292ms/step - loss: 0.0329 - accuracy: 0.9878 - val_loss: 0.4064 - val_accuracy: 0.9323\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 44, train: 41s, evaluation: 1s, total_time: 1882s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0329, metric: 0.9878\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.927740 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.670402 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.934237 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.836792 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 300ms/step - loss: 0.0315 - accuracy: 0.9887 - val_loss: 0.3987 - val_accuracy: 0.9201\n",
            "9/9 [==============================] - 1s 78ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 45, train: 41s, evaluation: 1s, total_time: 1925s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0315, metric: 0.9887\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.914161 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.656349 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.915605 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.818529 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 297ms/step - loss: 0.0320 - accuracy: 0.9882 - val_loss: 0.4208 - val_accuracy: 0.9284\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 46, train: 41s, evaluation: 2s, total_time: 1968s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0320, metric: 0.9882\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.923860 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.652706 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.930706 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.827431 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 299ms/step - loss: 0.0282 - accuracy: 0.9900 - val_loss: 0.4292 - val_accuracy: 0.9293\n",
            "9/9 [==============================] - 1s 80ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 47, train: 28s, evaluation: 2s, total_time: 1999s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0282, metric: 0.9900\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.924103 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.663653 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.929629 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.832104 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 297ms/step - loss: 0.0274 - accuracy: 0.9902 - val_loss: 0.4097 - val_accuracy: 0.9212\n",
            "9/9 [==============================] - 1s 79ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 48, train: 41s, evaluation: 2s, total_time: 2042s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0274, metric: 0.9902\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.915373 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.674826 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.915034 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.824430 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "95/95 [==============================] - 28s 291ms/step - loss: 0.0264 - accuracy: 0.9907 - val_loss: 0.4499 - val_accuracy: 0.9120\n",
            "9/9 [==============================] - 1s 81ms/step\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Epoch 49, train: 41s, evaluation: 2s, total_time: 2085s\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) [Train] loss: 0.0264, metric: 0.9907\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: acc = 0.905432 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs = 0.680613 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_hs_wei = 0.899454 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Evaluation on test data: f1_all = 0.815654 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Training:   1848 seconds in total\n",
            "\u001b[94m[INFO]\u001b[0m (__main__) Evaluation: 237 seconds in total\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) --------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) Best @ Epoch 20:\n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator) BestF1 0.706126 \n",
            "\u001b[94m[INFO]\u001b[0m (model_evaluator)   [TEST] report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.43      0.46       316\n",
            "           1       0.95      0.96      0.96      3808\n",
            "\n",
            "    accuracy                           0.92      4124\n",
            "   macro avg       0.72      0.70      0.71      4124\n",
            "weighted avg       0.92      0.92      0.92      4124\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS'\n",
        "output_path = ROOT_PROJECT+'/SKS/outputs/davidson/final'\n",
        "if not os.path.exists(output_path):\n",
        "  os.makedirs(output_path)\n",
        "!python DNN/train.py -d data/davidson/train_data.csv --trial data/davidson/test_data.csv -s data/sentiment_datasets/train_E6oV3lV.csv --word_list data/word_list/word_all.txt --emb data/glove.6B.300d.txt -o outputs/davidson/final -b 512 --epochs 50 --lr 0.002 --maxlen 50 -t HHMM_transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCbxayEaE6Ku"
      },
      "source": [
        "# **Gradio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAbEBcOXE8fo",
        "outputId": "f5ac00d3-3a2e-4294-e509-9410babd2ffa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1KUZ_9VBn505rUzXezbc-IZx3GjFFIvKL/Project_CS221/code/Project_CS221/SKS\n",
            "2023-12-25 13:39:59.267621: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-25 13:39:59.267689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-25 13:39:59.269625: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-25 13:39:59.280622: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-25 13:40:00.843982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-25 13:40:02.846443: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:02.896047: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:02.896386: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.096076: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.096435: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.096676: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.096846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-12-25 13:40:03.425955: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.426376: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.426590: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.427117: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.427360: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.427542: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.427798: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.427997: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-25 13:40:03.428194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://fd5b750ab49c93f37d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "%cd {ROOT_PROJECT}/'SKS'\n",
        "!python DNN/deploy_gradio.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Ej68d1Ug8f4Z",
        "KIQtciD-9fWr",
        "jBqFM7Z5A6UO",
        "4K5gOoGOBAQZ",
        "DCbxayEaE6Ku"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}